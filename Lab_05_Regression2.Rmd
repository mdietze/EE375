---
title: "Lab 05 - Regression II"
author: "EE375"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This lab is a continuation of last week’s exercise, where we were exploring the social and economic predictors of CO2 emissions. Last week we (1) generated hypotheses, (2) performed some exploratory analyses, and (3) fit a series of univariate linear regressions between our explanatory variables and our response (per capita CO2 emissions). In that analysis you likely found that most, if not all, of your proposed explanatory variables had “significant” relationships with per capita CO2 emissions, but that the amount of variability that each model explained was very different. In today’s lab you will build a multivariate regression model with these explanatory variables.

First, take a look back at Lab 04. You will need those results and that code to be able to proceed. In particular, you'll need to make sure you load up the data before proceeding

# Step 5: Co-linearity

Before testing possible multivariate models you will want to check your data for co-linearity between the explanatory variables.

1. Produce a `pairs` plot of your explanatory variables (i.e. scatterplots of the relationships among all the explanatory variables)
2. Include a table of the correlations **among your explanatory variables**
3. Are there variables that you would eliminate from a multivariate model based on co-linearity?

At this point, you know which variables will be in your final dataset. To ensure that you are working with a consistent set of data you’ll want to remove any rows from your data set that have an NA. If the set of variables you are testing is stored in the vector `my.variables`, and your dataframe is named `dat`, then you can do this using the function na.omit as follows

```
dat2 = na.omit(dat[,c(“Country.Code”, “EN.ATM.CO2E.PC”, my.variables)])
```

# Step 6: Multivariate analyses and model selection

At this stage we will use a forward selection approach to test progressively more complex models. At each stage you will want to add the variable that explains the most additional variability. In other words, if you have a model with two variables, `Y = b0 + b1*X1 + b2*X2`, and you have two other potential explanatory variables, Xa and Xb, you’ll first fit both possible three variable models:  `Y = b0 + b1*X1 + b2*X2 + b3*Xa` and `Y = b0 + b1*X1 + b2*X2 + b3*Xb`. You’ll then look to see which model has a higher R2. You will obviously stop adding variables when new variables are no longer significantly correlated (e.g. if b3 has a p-value > 0.05) or when you have exhausted all of your remaining candidate explanatory variables after checking for co-linearity (Step 5) and univariate correlations (Step 4). On the last point about univariate correlations, you generally don’t add a variable to a multivariate model that was not significant when it was tested by itself in a univariate regression model unless you have some insight or exploratory analysis that suggests that there really is a relationship but the impacts of the other covariates was masking that in the univariate regression.

4. Report the 'best' multivariate model at each level (e.g. the best models with 2, 3, and 4 variables) and describe the process you used to select each model and what statistics you used to justify these choices. Include the summary statistics from your initial univariate model, the final model, and the various models tried along the way.

# Step 7: Model Assessment

From this step forward you will be working with your best-fit models (both univariate and multivariate). **You should not need to refit any new models with lm to answer any of the following questions!**

Model assessment for a statistical model has a lot of similarities to how we assess _any_ model. For example, we might start by making plots of predicted vs. observed values -- in any model that's doing well points should fall along a 1:1 line. However, with a statistical model we have a few other tests that we need to perform to determine if the data conformed to the assumptions of the model, such as were the residuals Normal and do they have constant variance. 

For the overall best fit model:

5. Use the `predict` function to generate a plot of predicted vs observed per capita CO2 emissions (ie predicted CO2 emissions vs the data of CO2 emissions). Make sure to include a 1:1 line (a line with an intercept of 0 and slope of 1). Describe how the model performed based on this graph.

6. Use the `plot` function to generate diagnostic plots. Include the residual and Normal Q-Q plots. Did the data meet the normality and constant variance assumptions of the regression model?

# Step 8: Making Predictions

In this final section we’re going to apply our regression models forward in order to do a few useful things. First, we will use the best **univariate** model to predict CO2 emissions over a range of x values. Then we will use the best **multivariate** model predict emissions in the future given a scenario of your choice.

7. For the **best univariate model from Lab 04**, use `predict` to calculate the confidence and predictive intervals along a sequence of new x values that matches the range of the x axis. Generate a scatter plot that includes the regression line and these two intervals. (Hint: predict() takes an argument 'newdata'.  newdata should be a dataframe where the column-names are the same as the terms in lm() )

8. Using your **best fitting multivariate regression model**, predict (with CI and PI) what CO2 emissions will be over the next 25 years under a scenario of your choice. Choose one country for this analysis, and choose one of the variables in your multiple regression model to manipulate. For example: what if the life expectancy in Luxumborg increases by 1% per year? What if the GDP in Canada decreases by 2% per year? Keep things simple: assume all other variables in the multivariate stay constant, and the one that changes does so at a steady rate (for example %/year). Your final plot should have year on the X axis, and CO2 emissions on the x axis. 
